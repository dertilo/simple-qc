{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from keras.models import Sequential\n",
    "from sklearn.svm import LinearSVC\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import keras\n",
    "import nltk\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "\n",
    "patch_wordembedding_dt = '/home/eduardo/word_embedding/wiki.multi.nl.vec'\n",
    "patch_wordembedding_en = '/home/eduardo/word_embedding/wiki.multi.en.vec'\n",
    "patch_wordembedding_es = '/home/eduardo/word_embedding/wiki.multi.es.vec'\n",
    "patch_wordembedding_it = '/home/eduardo/word_embedding/wiki.multi.it.vec'\n",
    "patch_wordembedding_pt = '/home/eduardo/word_embedding/wiki.multi.pt.vec'\n",
    "\n",
    "embedding_dt = None\n",
    "embedding_en = None\n",
    "embedding_es = None\n",
    "embedding_it = None\n",
    "embedding_pt = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature(feature, df, df_2, embedding=None):\n",
    "    if feature == 'bow':\n",
    "        model = CountVectorizer(analyzer='word', strip_accents=None, \n",
    "                                ngram_range=(1, 1), lowercase=True, \n",
    "                                max_features=5000)\n",
    "        model.fit(df['question'])\n",
    "        ret = model.transform(df_2['question']).toarray()\n",
    "        df_2['bow'] = [x for x in ret]\n",
    "\n",
    "    if feature == 'tfidf':\n",
    "        model = TfidfVectorizer(analyzer='word', strip_accents=None, \n",
    "                                ngram_range=(1, 1), lowercase=True, \n",
    "                                max_features=5000)\n",
    "        model.fit(df['question'])\n",
    "        ret = model.transform(df_2['question']).toarray()\n",
    "        df_2['tfidf'] = [x for x in ret]\n",
    "\n",
    "    if feature == 'embedding':\n",
    "        if embedding is None:\n",
    "            print('Error: embedding None')\n",
    "            return\n",
    "        embds = []\n",
    "        for question in df_2['question']:\n",
    "            tokens = nltk.word_tokenize(question)\n",
    "            embed = []\n",
    "            for token in tokens:\n",
    "                if token.lower() in embedding:\n",
    "                    embed.append(embedding[token.lower()])\n",
    "                else:\n",
    "                    embed.append(np.zeros(300))\n",
    "            embds.append(embed)\n",
    "        df_2['embedding'] = embds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create supervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_linear():\n",
    "    return LinearSVC()\n",
    "\n",
    "def lstm_default(in_dim=300, out_dim=7, drop=0.3):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_dim=in_dim, name='0_LSTM'))\n",
    "    #model.add(Dropout(drop, name='1_Droupout'))\n",
    "    model.add(Dense(128, activation='relu', name='2_Dense'))\n",
    "    #model.add(Dropout(drop, name='3_Droupout'))\n",
    "    model.add(Dense(out_dim, activation='sigmoid', name='4_Dense'))\n",
    "    otimizer = keras.optimizers.Adam(lr=0.01) #decay = 0.0001\n",
    "    model.compile(optimizer=otimizer, loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(emb_path, nmax=80000):\n",
    "    embedding = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in embedding, 'word found twice'\n",
    "            embedding[word] = vect\n",
    "            if len(embedding) == nmax:\n",
    "                break\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nearst Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn(position, embedding, K=5):\n",
    "    rank = []\n",
    "    for word in embedding:\n",
    "        rank.append({'word': word, 'score': np.linalg.norm(position-embedding[word])})\n",
    "    list.sort(rank, key= lambda x: x['score'], reverse=True)\n",
    "    return rank[-K:][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark UIUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model, X, y, x_test, y_test, sizes_train=[], runs=30, save='default.csv', \n",
    "                  metric_average=\"macro\", onehot=None, out_dim=5, epochs=10):\n",
    "    start_benchmark = time.time()\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    for size_train in sizes_train:\n",
    "\n",
    "        print('\\n'+str(size_train), end='|')\n",
    "\n",
    "        for run in range(runs):\n",
    "            print('.', end='')\n",
    "            x_train = X[:size_train]\n",
    "            y_train = y[:size_train]\n",
    "\n",
    "            if 'lstm' in model['name']:\n",
    "                m = model['model'](out_dim=out_dim)\n",
    "                start_time = time.time()\n",
    "                m.fit(x_train, y_train, verbose=0, epochs=epochs)\n",
    "                train_time = time.time() - start_time\n",
    "                start_time = time.time()\n",
    "                result = m.predict(x_test)\n",
    "                test_time = time.time() - start_time\n",
    "                result = onehot.inverse_transform(result)\n",
    "                y_test_ = onehot.inverse_transform(y_test)\n",
    "            else:\n",
    "                m = model['model']()\n",
    "                start_time = time.time()\n",
    "                m.fit(x_train, y_train)\n",
    "                train_time = time.time() - start_time\n",
    "\n",
    "                start_time = time.time()\n",
    "                result = m.predict(x_test)\n",
    "                test_time = time.time() - start_time\n",
    "                y_test_ = y_test\n",
    "\n",
    "            data = {'datetime': datetime.datetime.now(),\n",
    "                    'model': model['name'],\n",
    "                    'accuracy': accuracy_score(result, y_test_),\n",
    "                    'precision': precision_score(result, y_test_, average=metric_average),\n",
    "                    'recall': recall_score(result, y_test_, average=metric_average),\n",
    "                    'f1': f1_score(result, y_test_, average=metric_average),\n",
    "                    'mcc': matthews_corrcoef(result, y_test_),\n",
    "                    'confusion': confusion_matrix(result, y_test_),\n",
    "                    'run': run + 1,\n",
    "                    'train_size': size_train,\n",
    "                    'execution_time': train_time,\n",
    "                    'test_time': test_time}\n",
    "            results = results.append([data])\n",
    "            results.to_csv(save)\n",
    "    print('')\n",
    "    aux = time.time() - start_benchmark\n",
    "    print('Run time benchmark:', aux)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_uiuc(language):\n",
    "    # language: 'en', 'pt', 'es'\n",
    "    return pd.read_csv('datasets/UIUC_' + language + '/train.csv'), pd.read_csv('datasets/UIUC_' + language + '/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run UIUC Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['manner', 'cremat', 'manner', ..., 'temp', 'temp', 'currency'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "1000|..............................\n",
      "2000|..............................\n",
      "3000|..............................\n",
      "4000|..............................\n",
      "5500|..............................\n",
      "Run time benchmark: 13.910954236984253\n",
      "\n",
      "1000|..............................\n",
      "2000|..............................\n",
      "3000|..............................\n",
      "4000|..............................\n",
      "5500|..............................\n",
      "Run time benchmark: 26.4685275554657\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "1000|..............................\n",
      "2000|..............................\n",
      "3000|..............................\n",
      "4000|..............................\n",
      "5500|..............................\n",
      "Run time benchmark: 17.02324151992798\n",
      "\n",
      "1000|..............................\n",
      "2000|..............................\n",
      "3000|..............................\n",
      "4000|..............................\n",
      "5500|..............................\n",
      "Run time benchmark: 31.185607433319092\n",
      "\n",
      "\n",
      "Language:  pt\n",
      "\n",
      "1000|..............................\n",
      "2000|..............................\n",
      "3000|..............................\n",
      "4000|..............................\n",
      "5500|..............................\n",
      "Run time benchmark: 14.749859094619751\n",
      "\n",
      "1000|..............................\n",
      "2000|..............................\n",
      "3000|..............................\n",
      "4000|..............................\n",
      "5500|..............................\n",
      "Run time benchmark: 27.339205265045166\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    # embedding = load_embedding('/home/eduardo/word_embedding/wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    create_feature('tfidf', dataset_train, dataset_train, embedding)\n",
    "    create_feature('tfidf', dataset_train, dataset_test, embedding)\n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    X_train = np.array([list(x) for x in dataset_train['tfidf'].values])\n",
    "    X_test = np.array([list(x) for x in dataset_test['tfidf'].values])\n",
    "    # X_train = pad_sequences(X_train, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    # X_test = pad_sequences(X_test, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "    y_train_sub = dataset_train['sub_class'].values\n",
    "    sub_classes = set()\n",
    "    for sc in y_train_sub:\n",
    "        sub_classes.add(sc)\n",
    "    y_test_sub = dataset_test['sub_class'].values\n",
    "    X_test_sub_ = []\n",
    "    y_test_sub_ = []\n",
    "    for i in range(len(X_test)):\n",
    "        if y_train_sub[i] in sub_classes:\n",
    "            X_test_sub_.append(X_test[i])\n",
    "            y_test_sub_.append(y_train_sub[i])\n",
    "    X_test_sub_ = np.array(X_test_sub_)\n",
    "    y_test_sub_ = np.array(y_test_sub_)\n",
    "    # ohe = OneHotEncoder()\n",
    "    # y_train = ohe.fit_transform([[y_] for y_ in y_train]).toarray()\n",
    "    # y_test = ohe.fit_transform([[y_] for y_ in y_test]).toarray()\n",
    "    # 1000, 2000, 3000, 4000\n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "                  save='results/UIUC_svm_tfidf_' + language + '.csv')\n",
    "    run_benchmark(model, X_train, y_train_sub, X_test_sub_, y_test_sub_, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "                  save='results/UIUCsub_svm_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000, 2000, 3000, 4000\n",
    "run_benchmark(models, X_train, y_train, X_test, y_test, sizes_train=[5500], save='results/UIUC_en_lstm_embedding_4.csv', onehot=ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISEQuA Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(mdeol, X, y, folds=10, save='default.csv', sizes_train=[],\n",
    "                  start_results=None, metric_average=\"macro\", onehot=None):\n",
    "    \n",
    "    start_benchmark = time.time()\n",
    "    results = pd.DataFrame()\n",
    "    for size_train in sizes_train:\n",
    "        print('\\n'+str(size_train)+'|', end='')\n",
    "        size_test = len(X) - size_train\n",
    "        rs = ShuffleSplit(n_splits=folds, train_size=size_train, test_size=size_test)\n",
    "        fold = 0\n",
    "        for train_indexs, test_indexs in rs.split(X):\n",
    "            print('.', end='')\n",
    "            x_train = X[train_indexs]\n",
    "            y_train = y[train_indexs]\n",
    "            x_test = X[test_indexs]\n",
    "            y_test = y[test_indexs]\n",
    "\n",
    "            if 'lstm' in model['name']:\n",
    "                m = model['model']()\n",
    "                start_time = time.time()\n",
    "                m.fit(x_train, y_train, verbose=0, epochs=100)\n",
    "                train_time = time.time() - start_time\n",
    "\n",
    "                start_time = time.time()\n",
    "                result = m.predict(x_test)\n",
    "                test_time = time.time() - start_time\n",
    "                result = onehot.inverse_transform(result)\n",
    "                y_test = onehot.inverse_transform(y_test)\n",
    "            else:\n",
    "                m = model['model']()\n",
    "                start_time = time.time()\n",
    "                m.fit(x_train, y_train)\n",
    "                train_time = time.time() - start_time\n",
    "\n",
    "                start_time = time.time()\n",
    "                result = m.predict(x_test)\n",
    "                test_time = time.time() - start_time\n",
    "\n",
    "            data = {'datetime': datetime.datetime.now(),\n",
    "                    'accuracy': accuracy_score(result, y_test),\n",
    "                    'precision': precision_score(result, y_test, average=metric_average),\n",
    "                    'recall': recall_score(result, y_test, average=metric_average),\n",
    "                    'f1': f1_score(result, y_test, average=metric_average),\n",
    "                    'confusion': confusion_matrix(result, y_test),\n",
    "                    'train_size': size_train,\n",
    "                    'fold': fold,\n",
    "                    'execution_time': train_time,\n",
    "                    'test_time': test_time}\n",
    "            results = results.append([data])\n",
    "            results.to_csv(save)\n",
    "            fold += 1\n",
    "    print('')\n",
    "    aux = time.time() - start_benchmark\n",
    "    print('Run time benchmark:', aux)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_disequa(language):\n",
    "    df = pd.read_csv('datasets/DISEQuA/disequa.csv')\n",
    "    return df[df['language'] == language]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN DISEQuA Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  DUT\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 0.705146074295044\n",
      "\n",
      "\n",
      "Language:  ENG\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 0.7089502811431885\n",
      "\n",
      "\n",
      "Language:  ITA\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 0.7117507457733154\n",
      "\n",
      "\n",
      "Language:  SPA\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 0.7464711666107178\n"
     ]
    }
   ],
   "source": [
    "for language in ['DUT', 'ENG', 'ITA', 'SPA']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    # embedding = load_embedding('/home/eduardo/word_embedding/wiki.multi.' + language + '.vec')\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf', dataset, dataset, embedding)\n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    X = np.array([list(x) for x in dataset['tfidf'].values])\n",
    "    y = dataset['class'].values\n",
    "    run_benchmark(model, X, y, sizes_train=[100,200,300,400],\n",
    "                  save='results/DISEQuA_svm_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_benchmark(models, X, y, sizes_train=[100,200,300,400], save='results/DISEQuA_SPA_lstm_embedding.csv', onehot=ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa",
   "language": "python",
   "name": "qa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
