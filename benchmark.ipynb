{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Bidirectional, MaxPooling1D, Flatten, concatenate, GlobalMaxPooling1D, Concatenate\n",
    "from keras.layers import Dense, Dropout, LSTM, TimeDistributed,Conv1D,Embedding, Reshape, Conv2D, MaxPool2D\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import LinearSVC, SVR\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from collections import Counter\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "import random\n",
    "import keras\n",
    "import nltk\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "\n",
    "patch_wordembedding_dt = '/home/eduardo/word_embedding/wiki.multi.nl.vec'\n",
    "patch_wordembedding_en = '/home/eduardo/word_embedding/wiki.multi.en.vec'\n",
    "patch_wordembedding_es = '/home/eduardo/word_embedding/wiki.multi.es.vec'\n",
    "patch_wordembedding_it = '/home/eduardo/word_embedding/wiki.multi.it.vec'\n",
    "patch_wordembedding_pt = '/home/eduardo/word_embedding/wiki.multi.pt.vec'\n",
    "\n",
    "embedding_dt = None\n",
    "embedding_en = None\n",
    "embedding_es = None\n",
    "embedding_it = None\n",
    "embedding_pt = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature(feature, df, df_2, embedding=None):\n",
    "    if feature == 'bow':\n",
    "        model = CountVectorizer(analyzer='word', strip_accents=None, \n",
    "                                ngram_range=(1, 1), lowercase=True, \n",
    "                                max_features=5000)\n",
    "        model.fit(df['question'])\n",
    "        ret = model.transform(df_2['question']).toarray()\n",
    "        df_2['bow'] = [x for x in ret]\n",
    "\n",
    "    if feature == 'tfidf':\n",
    "        model = TfidfVectorizer(analyzer='word', strip_accents=None, \n",
    "                                ngram_range=(1, 1), lowercase=True, \n",
    "                                max_features=5000)\n",
    "        model.fit(df['question'])\n",
    "        ret = model.transform(df_2['question']).toarray()\n",
    "        df_2['tfidf'] = [x for x in ret]\n",
    "    \n",
    "    if feature == 'tfidf_3gram':\n",
    "        model = TfidfVectorizer(analyzer='word', strip_accents=None, \n",
    "                                ngram_range=(1, 2), lowercase=True, \n",
    "                                max_features=80000)\n",
    "        model.fit(df['question'])\n",
    "        ret = model.transform(df_2['question']).toarray()\n",
    "        df_2['tfidf'] = [x for x in ret]\n",
    "\n",
    "    if feature == 'embedding':\n",
    "        if embedding is None:\n",
    "            print('Error: embedding None')\n",
    "            return\n",
    "        embds = []\n",
    "        for question in df_2['question']:\n",
    "            tokens = nltk.word_tokenize(question)\n",
    "            embed = []\n",
    "            for token in tokens:\n",
    "                if token.lower() in embedding:\n",
    "                    embed.append(embedding[token.lower()])\n",
    "                else:\n",
    "                    embed.append(np.zeros(300))\n",
    "            embds.append(embed)\n",
    "        df_2['embedding'] = embds\n",
    "        \n",
    "    if feature == 'vocab_index':\n",
    "        questions_split = [nltk.word_tokenize(q) for q in df['question']]\n",
    "        questions_split_2 = [nltk.word_tokenize(q) for q in df_2['question']]\n",
    "        padding_word = \"<PAD/>\"\n",
    "        padded_questions = []\n",
    "        padded_questions_2 = []\n",
    "        sequence_length = max(len(x) for x in questions_split)\n",
    "        for i in range(len(questions_split)):\n",
    "            question = questions_split[i]\n",
    "            num_padding = sequence_length - len(question)\n",
    "            new_question = question + [padding_word] * num_padding\n",
    "            padded_questions.append(new_question)\n",
    "        word_counts = Counter(itertools.chain(*padded_questions))\n",
    "        vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "        vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "        vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "        for i in range(len(questions_split_2)):\n",
    "            question = questions_split_2[i]\n",
    "            nq = []\n",
    "            for w in question:\n",
    "                if w in vocabulary:\n",
    "                    nq.append(w)\n",
    "                else:\n",
    "                    nq.append(padding_word)\n",
    "            question = nq\n",
    "            num_padding = sequence_length - len(question)\n",
    "            new_question = question + [padding_word] * num_padding\n",
    "            padded_questions_2.append(new_question)\n",
    "        \n",
    "        print('ok')\n",
    "        df_2['vocab_index'] = [[vocabulary[word] for word in question] for question in padded_questions_2]\n",
    "        print('ok_2')\n",
    "        return vocabulary_inv\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create supervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_linear():\n",
    "    return LinearSVC()\n",
    "\n",
    "def lstm_default(in_dim=300, out_dim=7, drop=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_dim=in_dim, name='0_LSTM'))\n",
    "    model.add(Dropout(drop, name='1_Droupout'))\n",
    "    model.add(Dense(128, activation='relu', name='2_Dense'))\n",
    "    model.add(Dropout(drop, name='3_Droupout'))\n",
    "    model.add(Dense(out_dim, activation='softmax', name='4_Dense'))\n",
    "    otimizer = keras.optimizers.Adam(lr=0.01) #decay = 0.0001\n",
    "    model.compile(optimizer=otimizer, loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "def random_forest():\n",
    "    return RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "def mlp(in_dim=5000, out_dim=7, drop=0.65):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=in_dim, activation='relu'))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(out_dim, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def cnn(sequence_length, vocabulary_size, embedding_dim=300, filter_sizes=[3,4,5], num_filters=512, drop=0.5, out_dim=7):\n",
    "    inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "    embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "    reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "    \n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "    maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "    \n",
    "    concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "    flatten = Flatten()(concatenated_tensor)\n",
    "    dropout = Dropout(drop)(flatten)\n",
    "    output = Dense(units=out_dim, activation='softmax')(dropout)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(emb_path, nmax=50000):\n",
    "    embedding = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in embedding, 'word found twice'\n",
    "            embedding[word] = vect\n",
    "            if len(embedding) == nmax:\n",
    "                break\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nearst Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn(position, embedding, K=5):\n",
    "    rank = []\n",
    "    for word in embedding:\n",
    "        rank.append({'word': word, 'score': np.linalg.norm(position-embedding[word])})\n",
    "    list.sort(rank, key= lambda x: x['score'], reverse=True)\n",
    "    return rank[-K:][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark UIUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model, X, y, x_test, y_test, sizes_train=[], runs=30, save='default.csv', \n",
    "                  metric_average=\"macro\", onehot=None, out_dim=6, epochs=10, batch_size=30, vocabulary_size=5000):\n",
    "    start_benchmark = time.time()\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    for size_train in sizes_train:\n",
    "\n",
    "        print('\\n'+str(size_train), end='|')\n",
    "\n",
    "        for run in range(runs):\n",
    "            print('.', end='')\n",
    "            x_train = X[:size_train]\n",
    "            y_train = y[:size_train]\n",
    "\n",
    "            if 'lstm' in model['name'] or 'mlp' in model['name']:\n",
    "                m = model['model'](out_dim=len(onehot.categories_[0]))\n",
    "                start_time = time.time()\n",
    "                m.fit(x_train, y_train, verbose=1, epochs=epochs)\n",
    "                train_time = time.time() - start_time\n",
    "                start_time = time.time()\n",
    "                result = m.predict(x_test)\n",
    "                test_time = time.time() - start_time\n",
    "                #print('  nan:', np.any(np.isnan(result)), end='')\n",
    "                #print('  fin:', np.all(np.isfinite(result)), end='')\n",
    "                result = np.nan_to_num(result)\n",
    "                result = onehot.inverse_transform(result)\n",
    "                y_test_ = onehot.inverse_transform(y_test)\n",
    "            elif 'cnn' in model['name']:\n",
    "                sequence_length = x_train.shape[1]\n",
    "                out_dim = len(onehot.categories_[0])\n",
    "                m = model['model'](sequence_length, vocabulary_size, out_dim=out_dim)\n",
    "                start_time = time.time()\n",
    "                m.fit(x_train, y_train, verbose=0, epochs=epochs, batch_size=batch_size)\n",
    "                train_time = time.time() - start_time\n",
    "                start_time = time.time()\n",
    "                result = m.predict(x_test)\n",
    "                test_time = time.time() - start_time\n",
    "                #print('  nan:', np.any(np.isnan(result)), end='')\n",
    "                #print('  fin:', np.all(np.isfinite(result)), end='')\n",
    "                result = np.nan_to_num(result)\n",
    "                result = onehot.inverse_transform(result)\n",
    "                y_test_ = onehot.inverse_transform(y_test)\n",
    "            else:\n",
    "                m = model['model']()\n",
    "                start_time = time.time()\n",
    "                m.fit(x_train, y_train)\n",
    "                train_time = time.time() - start_time\n",
    "\n",
    "                start_time = time.time()\n",
    "                result = m.predict(x_test)\n",
    "                test_time = time.time() - start_time\n",
    "                y_test_ = y_test\n",
    "\n",
    "            data = {'datetime': datetime.datetime.now(),\n",
    "                    'model': model['name'],\n",
    "                    'accuracy': accuracy_score(result, y_test_),\n",
    "                    'precision': precision_score(result, y_test_, average=metric_average),\n",
    "                    'recall': recall_score(result, y_test_, average=metric_average),\n",
    "                    'f1': f1_score(result, y_test_, average=metric_average),\n",
    "                    'mcc': matthews_corrcoef(result, y_test_),\n",
    "                    'confusion': confusion_matrix(result, y_test_),\n",
    "                    'run': run + 1,\n",
    "                    'train_size': size_train,\n",
    "                    'execution_time': train_time,\n",
    "                    'test_time': test_time}\n",
    "            results = results.append([data])\n",
    "            results.to_csv(save)\n",
    "    print('')\n",
    "    aux = time.time() - start_benchmark\n",
    "    print('Run time benchmark:', aux)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_uiuc(language):\n",
    "    # language: 'en', 'pt', 'es'\n",
    "    return pd.read_csv('datasets/UIUC_' + language + '/train.csv'), pd.read_csv('datasets/UIUC_' + language + '/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run UIUC Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "ok\n",
      "ok_2\n",
      "ok\n",
      "ok_2\n",
      "\n",
      "1000|..............................\n",
      "2000|..............................\n",
      "3000|..............................\n",
      "4000|..............................\n",
      "5500|..............................\n",
      "Run time benchmark: 202189.5595524311\n",
      "\n",
      "\n",
      "Language:  pt\n",
      "ok\n",
      "ok_2\n",
      "ok\n",
      "ok_2\n",
      "\n",
      "1000|..............................\n",
      "2000|..............................\n",
      "3000|..............................\n",
      "4000|......................."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-667d6f80599c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500],\n\u001b[1;32m     19\u001b[0m                   \u001b[0mruns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'results/UIUC_cnn_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monehot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mohe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                   vocabulary_size=len(vocabulary_inv))\n\u001b[0m",
      "\u001b[0;32m<ipython-input-207-4bed4a05790f>\u001b[0m in \u001b[0;36mrun_benchmark\u001b[0;34m(model, X, y, x_test, y_test, sizes_train, runs, save, metric_average, onehot, out_dim, epochs, batch_size, vocabulary_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/qa/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.virtualenvs/qa/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/qa/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/qa/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m         \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/qa/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for language in ['en', 'pt', 'es']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    #embedding = load_embedding('/home/eduardo/word_embedding/wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    text_representation = 'vocab_index'\n",
    "    vocabulary_inv = create_feature(text_representation, dataset_train, dataset_train)\n",
    "    create_feature(text_representation, dataset_train, dataset_test)\n",
    "    model = {'name': 'cnn', 'model': cnn}\n",
    "    X_train = np.array([list(x) for x in dataset_train[text_representation].values])\n",
    "    X_test = np.array([list(x) for x in dataset_test[text_representation].values])\n",
    "    #X_train = pad_sequences(X_train, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    #X_test = pad_sequences(X_test, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "    ohe = OneHotEncoder()\n",
    "    y_train = ohe.fit_transform([[y_] for y_ in y_train]).toarray()\n",
    "    y_test = ohe.transform([[y_] for y_ in y_test]).toarray()\n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "                  runs=30, save='results/UIUC_cnn_' + language + '.csv', epochs=100, onehot=ohe,\n",
    "                  vocabulary_size=len(vocabulary_inv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM + WordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in ['pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding('/home/eduardo/word_embedding/wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    create_feature('embedding', dataset_train, dataset_train, embedding)\n",
    "    create_feature('embedding', dataset_train, dataset_test, embedding)\n",
    "    model = {'name': 'lstm', 'model': lstm_default}\n",
    "    X_train = np.array([list(x) for x in dataset_train['embedding'].values])\n",
    "    X_test = np.array([list(x) for x in dataset_test['embedding'].values])\n",
    "    X_train = pad_sequences(X_train, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    X_test = pad_sequences(X_test, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "#     y_train_sub = dataset_train['sub_class'].values\n",
    "#     sub_classes = set()\n",
    "#     for sc in y_train_sub:\n",
    "#         sub_classes.add(sc)\n",
    "#     y_test_sub = dataset_test['sub_class'].values\n",
    "#     X_test_sub_ = []\n",
    "#     y_test_sub_ = []\n",
    "#     for i in range(len(X_test)):\n",
    "#         if y_train_sub[i] in sub_classes:\n",
    "#             X_test_sub_.append(X_test[i])\n",
    "#             y_test_sub_.append(y_train_sub[i])\n",
    "#     X_test_sub_ = np.array(X_test_sub_)\n",
    "#     y_test_sub_ = np.array(y_test_sub_)\n",
    "    ohe = OneHotEncoder()\n",
    "    y_train = ohe.fit_transform([[y_] for y_ in y_train]).toarray()\n",
    "    y_test = ohe.transform([[y_] for y_ in y_test]).toarray() \n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "                  runs=30, save='results/UIUC_lstm_embedding_' + language + '.csv', epochs=100, onehot=ohe)\n",
    "    #run_benchmark(model, X_train, y_train_sub, X_test_sub_, y_test_sub_, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "    #              save='results/UIUCsub_svm_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    create_feature('tfidf', dataset_train, dataset_train, embedding)\n",
    "    create_feature('tfidf', dataset_train, dataset_test, embedding)\n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    X_train = np.array([list(x) for x in dataset_train['tfidf'].values])\n",
    "    X_test = np.array([list(x) for x in dataset_test['tfidf'].values])\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "                  save='results/UIUC_svm_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFC + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    create_feature('tfidf', dataset_train, dataset_train, embedding)\n",
    "    create_feature('tfidf', dataset_train, dataset_test, embedding)\n",
    "    model = {'name': 'rfc', 'model': random_forest}\n",
    "    X_train = np.array([list(x) for x in dataset_train['tfidf'].values])\n",
    "    X_test = np.array([list(x) for x in dataset_test['tfidf'].values])\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "                  save='results/UIUC_rfc_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + TFIDF_3gram + RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "SKB Start\n",
      "SKB End\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 1.2348272800445557\n",
      "\n",
      "\n",
      "Language:  es\n",
      "SKB Start\n",
      "SKB End\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 1.5204951763153076\n",
      "\n",
      "\n",
      "Language:  pt\n",
      "SKB Start\n",
      "SKB End\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 1.346195936203003\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    create_feature('tfidf_3gram', dataset_train, dataset_train)\n",
    "    create_feature('tfidf_3gram', dataset_train, dataset_test)\n",
    "    model = {'name': 'svm_skb', 'model': svm_linear}\n",
    "    X_train = np.array([list(x) for x in dataset_train['tfidf'].values])\n",
    "    X_test = np.array([list(x) for x in dataset_test['tfidf'].values])\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "    \n",
    "    classes = list(dataset_train['class'].unique())\n",
    "    y_train_ = [classes.index(c) for c in y_train]\n",
    "    \n",
    "    print('SKB Start')\n",
    "    #selector = selector.fit(X_train, y_train_)\n",
    "    skb = SelectKBest(chi2, k=5000).fit(X_train, y_train_)\n",
    "    X_train = skb.transform(X_train)\n",
    "    X_test = skb.transform(X_test)\n",
    "    print('SKB End')\n",
    "    \n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "                  runs=1, save='results/UIUC_svm_skb_tfidf3gram_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISEQuA Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model, X, y, folds=10, save='default.csv', sizes_train=[],\n",
    "                  start_results=None, metric_average=\"macro\", onehot=None):\n",
    "    \n",
    "    start_benchmark = time.time()\n",
    "    results = pd.DataFrame()\n",
    "    for size_train in sizes_train:\n",
    "        print('\\n'+str(size_train)+'|', end='')\n",
    "        size_test = len(X) - size_train\n",
    "        rs = ShuffleSplit(n_splits=folds, train_size=size_train, test_size=size_test)\n",
    "        fold = 0\n",
    "        for train_indexs, test_indexs in rs.split(X):\n",
    "            print('.', end='')\n",
    "            x_train = X[train_indexs]\n",
    "            y_train = y[train_indexs]\n",
    "            x_test = X[test_indexs]\n",
    "            y_test = y[test_indexs]\n",
    "\n",
    "            if 'lstm' in model['name']:\n",
    "                m = model['model']()\n",
    "                start_time = time.time()\n",
    "                m.fit(x_train, y_train, verbose=0, epochs=100)\n",
    "                train_time = time.time() - start_time\n",
    "\n",
    "                start_time = time.time()\n",
    "                result = m.predict(x_test)\n",
    "                test_time = time.time() - start_time\n",
    "                result = onehot.inverse_transform(result)\n",
    "                y_test = onehot.inverse_transform(y_test)\n",
    "            else:\n",
    "                m = model['model']()\n",
    "                start_time = time.time()\n",
    "                m.fit(x_train, y_train)\n",
    "                train_time = time.time() - start_time\n",
    "\n",
    "                start_time = time.time()\n",
    "                result = m.predict(x_test)\n",
    "                test_time = time.time() - start_time\n",
    "\n",
    "            data = {'datetime': datetime.datetime.now(),\n",
    "                    'accuracy': accuracy_score(result, y_test),\n",
    "                    'precision': precision_score(result, y_test, average=metric_average),\n",
    "                    'recall': recall_score(result, y_test, average=metric_average),\n",
    "                    'f1': f1_score(result, y_test, average=metric_average),\n",
    "                    'confusion': confusion_matrix(result, y_test),\n",
    "                    'train_size': size_train,\n",
    "                    'fold': fold,\n",
    "                    'execution_time': train_time,\n",
    "                    'test_time': test_time}\n",
    "            results = results.append([data])\n",
    "            results.to_csv(save)\n",
    "            fold += 1\n",
    "    print('')\n",
    "    aux = time.time() - start_benchmark\n",
    "    print('Run time benchmark:', aux)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_disequa(language):\n",
    "    df = pd.read_csv('datasets/DISEQuA/disequa.csv')\n",
    "    return df[df['language'] == language]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN DISEQuA Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in ['DUT', 'ENG', 'ITA', 'SPA']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf', dataset, dataset, embedding)\n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    X = np.array([list(x) for x in dataset['tfidf'].values])\n",
    "    y = dataset['class'].values\n",
    "    run_benchmark(model, X, y, sizes_train=[100,200,300,400],\n",
    "                  save='results/DISEQuA_svm_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RFC + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in ['DUT', 'ENG', 'ITA', 'SPA']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf', dataset, dataset, embedding)\n",
    "    model = {'name': 'rfc', 'model': random_forest}\n",
    "    X = np.array([list(x) for x in dataset['tfidf'].values])\n",
    "    y = dataset['class'].values\n",
    "    run_benchmark(model, X, y, sizes_train=[100,200,300,400],\n",
    "                  save='results/DISEQuA_rfc_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM + TFIDF_3gram + SKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  DUT\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 1.1172964572906494\n",
      "\n",
      "\n",
      "Language:  ENG\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 1.2064998149871826\n",
      "\n",
      "\n",
      "Language:  ITA\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|.........."
     ]
    }
   ],
   "source": [
    "for language in ['DUT', 'ENG', 'ITA', 'SPA']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf_3gram', dataset, dataset)\n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    X = np.array([list(x) for x in dataset['tfidf'].values])\n",
    "    y = dataset['class'].values\n",
    "    skb = SelectKBest(chi2, k=2000).fit(X, y)\n",
    "    X = skb.transform(X)\n",
    "    run_benchmark(model, X, y, sizes_train=[100,200,300,400],\n",
    "                  save='results/DISEQuA_svm_tfidf_3gram_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM + Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language, embd_l in zip(['SPA'], ['es']):\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding('/home/eduardo/word_embedding/wiki.multi.' + embd_l + '.vec')\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('embedding', dataset, dataset, embedding)\n",
    "    model = {'name': 'lstm', 'model': lstm_default}\n",
    "    X = np.array([list(x) for x in dataset['embedding'].values])\n",
    "    y = dataset['class'].values\n",
    "    X = pad_sequences(X, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    ohe = OneHotEncoder()\n",
    "    y = ohe.fit_transform([[y_] for y_ in y]).toarray()\n",
    "    run_benchmark(model, X, y, sizes_train=[100,200,300,400], onehot=ohe,\n",
    "                  save='results/DISEQuA_lstm_embedding_' + language + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa",
   "language": "python",
   "name": "qa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
